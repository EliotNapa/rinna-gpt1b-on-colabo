{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#rinna/japanese-gpt1bのcolabでの実行環境。\n","セルを1ステップずつ実行してください。\\\n","左にチェックマークが表示され、実行が完了したら次のセルを実行してください。"],"metadata":{"id":"f_I9WoHpo2XT"}},{"cell_type":"code","source":["! nvidia-smi\n","! nvcc -V\n","! free -h\n","! lsb_release -a"],"metadata":{"id":"kRdl9Ug24A5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers==4.4.2\n","!pip install sentencepiece==0.1.91\n","!pip install torch"],"metadata":{"id":"aCodYtf-4fuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import T5Tokenizer, AutoModelForCausalLM\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt-1b\")\n","model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-1b\")\n","\n","if torch.cuda.is_available():\n","    model = model.to(\"cuda\")"],"metadata":{"id":"lyWZaCTp5HJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##推論の実行\n","この下のセルを実行する前にtextに推論のもとにする文章を入力してください。\\\n","推論を繰り返す場合は、textを変更して、下のセルを実行し直してください。"],"metadata":{"id":"5Gw1obCWpEQ2"}},{"cell_type":"code","source":["import textwrap\n","#サンプルは夏目漱石／吾輩は猫である\n","text = \"吾輩は猫である。名前はまだ無い。\"\n","token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    output_ids = model.generate(\n","        token_ids.to(model.device),\n","        max_length=100,\n","        min_length=100,\n","        do_sample=True,\n","        top_k=500,\n","        top_p=0.95,\n","        pad_token_id=tokenizer.pad_token_id,\n","        bos_token_id=tokenizer.bos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","        bad_word_ids=[[tokenizer.unk_token_id]],\n","        num_return_sequences=3\n","    )\n","\n","#output = tokenizer.decode(output_ids.tolist()[0])\n","#print(tokenizer.decode(output_ids.tolist()[1]))\n","#print(output)\n","answer = tokenizer.batch_decode(output_ids)\n","for line in answer:\n","  one_lines = textwrap.wrap(line, 40)\n","  for sent in one_lines:\n","    print(sent)\n","  print(\"\")\n"],"metadata":{"id":"57h21mBu5Pwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"F5gIEB9N6SF1"},"execution_count":null,"outputs":[]}]}